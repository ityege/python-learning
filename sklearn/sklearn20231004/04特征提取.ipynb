{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22236025-ed35-4f6e-af93-cf58e54cc8dc",
   "metadata": {},
   "source": [
    "## 4.1　从类别变量中提取特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b721fd3-c17c-4246-a505-ecc6cbdb5428",
   "metadata": {},
   "source": [
    "代码4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9dab329-27c9-4770-aea0-cb018eaed7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "onehot_encoder = DictVectorizer()\n",
    "X= [\n",
    "   {'city': 'New York'},\n",
    "   {'city': 'San Francisco'},\n",
    "   {'city': 'Chapel Hill'}\n",
    "]\n",
    "print(onehot_encoder.fit_transform(X).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaf2453-5e8a-43bb-bfa2-22b0291e264a",
   "metadata": {},
   "source": [
    "## 4.2　特征标准化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c542f-07d4-451f-a60c-0ad8cef04e1f",
   "metadata": {},
   "source": [
    "代码4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e34c9d-c6fe-4fca-be44-0a563cc7f496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.70710678 -1.38873015  0.52489066  0.59299945 -1.35873244]\n",
      " [ 0.         -0.70710678  0.46291005  0.87481777  0.81537425  1.01904933]\n",
      " [ 0.          1.41421356  0.9258201  -1.39970842 -1.4083737   0.33968311]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "X = np.array([\n",
    " [0., 0., 5., 13., 9., 1.],\n",
    " [0., 0., 13., 15., 10., 15.],\n",
    " [0., 3., 15., 2., 0., 11.]\n",
    "])\n",
    "print(preprocessing.scale(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f379ddb5-e15c-4336-a8d6-1cb4feb74f02",
   "metadata": {},
   "source": [
    "## 4.3　从文本中提取特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3de6aa9-d006-4d9d-b5ac-6d016bf67bc8",
   "metadata": {},
   "source": [
    "### 4.3.1　词袋模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dbe900-c8fb-4693-a36b-47fe77a851ae",
   "metadata": {},
   "source": [
    "代码4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0db367-66fc-40c9-a398-e035cd1d1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8938c90a-e6a1-4969-8195-bb5cfd2f6029",
   "metadata": {},
   "source": [
    "代码4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d3817f5-ffc0-4467-9e68-a67f92b993ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 1 0]]\n",
      "{'unc': 7, 'played': 5, 'duke': 1, 'in': 3, 'basketball': 0, 'lost': 4, 'the': 6, 'game': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04118607-c690-41b9-a8dd-464f7e8cfb32",
   "metadata": {},
   "source": [
    "代码4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94da8619-798a-4c16-8f57-572136d3c319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0 0]]\n",
      "{'unc': 9, 'played': 6, 'duke': 2, 'in': 4, 'basketball': 1, 'lost': 5, 'the': 8, 'game': 3, 'ate': 0, 'sandwich': 7}\n"
     ]
    }
   ],
   "source": [
    "corpus.append('I ate a sandwich')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbd7f7-9bc0-4ce4-a98b-1be6b1c50e6e",
   "metadata": {},
   "source": [
    "代码4.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337123e7-3a72-465c-9790-9590e7bb7639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between 1st and 2nd documents: [[2.44948974]]\n",
      "Distance between 1st and 3rd documents: [[2.64575131]]\n",
      "Distance between 2nd and 3rd documents: [[2.64575131]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "X = vectorizer.fit_transform(corpus).todense()\n",
    "X=np.array(X)\n",
    "print('Distance between 1st and 2nd documents:',euclidean_distances(X[0].reshape(1, -1), X[1].reshape(1, -1)))\n",
    "print('Distance between 1st and 3rd documents:', euclidean_distances(X[0].reshape(1, -1), X[2].reshape(1, -1)))\n",
    "print('Distance between 2nd and 3rd documents:',euclidean_distances(X[1].reshape(1, -1), X[2].reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38945f69-b6b7-4e0e-9969-825cf43bb654",
   "metadata": {},
   "source": [
    "### 4.3.2　停用词过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef1a2c-ae9f-4404-bc6d-370bd70d256e",
   "metadata": {},
   "source": [
    "代码4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18420b63-a8a3-4165-9e89-de0e59b3f4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 0 1]\n",
      " [0 1 1 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 0]]\n",
      "{'unc': 7, 'played': 5, 'duke': 2, 'basketball': 1, 'lost': 4, 'game': 3, 'ate': 0, 'sandwich': 6}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67156117-3bf1-439f-85b7-73a78afd0763",
   "metadata": {},
   "source": [
    "### 4.3.3　词干提取和词形还原"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ac35a-431d-4909-8c1c-bd22af27f350",
   "metadata": {},
   "source": [
    "代码4.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef9ae2be-43ba-408d-97b8-bb6c1b2b32d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "{'ate': 0, 'sandwiches': 3, 'sandwich': 2, 'eaten': 1}\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'He ate the sandwiches','Every sandwich was eaten by him'\n",
    "]\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db8e991-0d49-423b-84c1-be0bfb92ce91",
   "metadata": {},
   "source": [
    "代码4.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "394365d1-abe4-4d08-bcad-6f01bbef018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'I am gathering ingredients for the sandwich.',\n",
    "    'There were many wizards at the gathering.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de7ffa0-d9bf-4830-a3b7-c617cbe8ecf4",
   "metadata": {},
   "source": [
    "代码4.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c90a45e4-1026-49c9-8ef6-878cb7c84990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\liye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "789088bb-4083-45aa-bb3f-3d066c5d45b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather\n",
      "gathering\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('gathering', 'v'))\n",
    "print(lemmatizer.lemmatize('gathering', 'n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ae6db1-9150-4577-adbe-d77a6f15baf3",
   "metadata": {},
   "source": [
    "代码4.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "617fec2c-bc61-4ae7-9ae3-c301df19aa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('gathering'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420c6ca-feeb-46d5-9caf-fbaa35f85934",
   "metadata": {},
   "source": [
    "代码4.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "981d52a1-0d63-497d-84e5-c6cd92bdf938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\liye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\liye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56b03d64-dae7-49bf-b65c-9ef561ac2a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed: [['he', 'ate', 'the', 'sandwich'], ['everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]\n",
      "Lemmatized: [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "wordnet_tags = ['n', 'v']\n",
    "corpus = [\n",
    "    'He ate the sandwiches',\n",
    "    'Every sandwich was eaten by him'\n",
    "]\n",
    "stemmer = PorterStemmer()\n",
    "print('Stemmed:', [[stemmer.stem(token) for token in\n",
    "word_tokenize(document)] for document in corpus])\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    if tag[0].lower() in ['n', 'v']:\n",
    "        return lemmatizer.lemmatize(token, tag[0].lower())\n",
    "    return token\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tagged_corpus = [pos_tag(word_tokenize(document)) for document in\n",
    "  corpus]\n",
    "print('Lemmatized:', [[lemmatize(token, tag) for token, tag in\n",
    "   document] for document in tagged_corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48c5337-d38f-48f3-9e07-0b363feead82",
   "metadata": {},
   "source": [
    "### 4.3.4　tf-idf权重扩展词包"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32f71d-60b1-4376-9e0f-93c9ff7e86ff",
   "metadata": {},
   "source": [
    "代码4.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcb30ef5-8a7a-42a3-b758-44bbd6ec83c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 3 1 1]\n",
      "Token indices {'dog': 1, 'ate': 0, 'sandwich': 2, 'wizard': 4, 'transfigured': 3}\n",
      "The token \"dog\" appears 1 times\n",
      "The token \"ate\" appears 2 times\n",
      "The token \"sandwich\" appears 3 times\n",
      "The token \"wizard\" appears 1 times\n",
      "The token \"transfigured\" appears 1 times\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['The dog ate a sandwich, the wizard transfigured a sandwich,and I ate a sandwich']\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "frequencies = np.array(vectorizer.fit_transform(corpus).todense())[0]\n",
    "print(frequencies)\n",
    "print('Token indices %s' % vectorizer.vocabulary_)\n",
    "for token, index in vectorizer.vocabulary_.items():\n",
    "    print('The token \"%s\" appears %s times' % (token,\n",
    "      frequencies[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b7290-d72c-4b57-ae53-756802cc70ac",
   "metadata": {},
   "source": [
    "代码4.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0a67610-54c2-468e-86c1-84bd7540f2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.75458397 0.37729199 0.53689271 0.         0.        ]\n",
      " [0.         0.         0.44943642 0.6316672  0.6316672 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'The dog ate a sandwich and I ate a sandwich',\n",
    "    'The wizard transfigured a sandwich'\n",
    "]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da7c77d-4076-4006-93f4-2e7e3c4d27fe",
   "metadata": {},
   "source": [
    "### 4.3.5　空间有效特征向量化与哈希技巧"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019940ef-49ae-42fd-b873-c8b03a303379",
   "metadata": {},
   "source": [
    "代码4.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63dd3a9c-3b6f-47e3-8836-dd0701d9715b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "corpus = ['the', 'ate', 'bacon', 'cat']\n",
    "vectorizer = HashingVectorizer(n_features=6)\n",
    "print(vectorizer.transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8542c193-ab43-44bf-8842-eaecf81bd300",
   "metadata": {},
   "source": [
    "### 4.3.6　词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed539a20-84bb-42e0-a2d9-04f6395c755d",
   "metadata": {},
   "source": [
    "代码4.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7be990b0-973d-4ed9-b914-ee66d222c316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: 300\n",
      "[ 0.0123291   0.20410156 -0.28515625  0.21679688  0.11816406  0.08300781\n",
      "  0.04980469 -0.00952148  0.22070312 -0.12597656  0.08056641 -0.5859375\n",
      " -0.00445557 -0.296875   -0.01312256 -0.08349609  0.05053711  0.15136719\n",
      " -0.44921875 -0.0135498   0.21484375 -0.14746094  0.22460938 -0.125\n",
      " -0.09716797  0.24902344 -0.2890625   0.36523438  0.41210938 -0.0859375\n",
      " -0.07861328 -0.19726562 -0.09082031 -0.14160156 -0.10253906  0.13085938\n",
      " -0.00346375  0.07226562  0.04418945  0.34570312  0.07470703 -0.11230469\n",
      "  0.06738281  0.11230469  0.01977539 -0.12353516  0.20996094 -0.07226562\n",
      " -0.02783203  0.05541992 -0.33398438  0.08544922  0.34375     0.13964844\n",
      "  0.04931641 -0.13476562  0.16308594 -0.37304688  0.39648438  0.10693359\n",
      "  0.22167969  0.21289062 -0.08984375  0.20703125  0.08935547 -0.08251953\n",
      "  0.05957031  0.10205078 -0.19238281 -0.09082031  0.4921875   0.03955078\n",
      " -0.07080078 -0.0019989  -0.23046875  0.25585938  0.08984375 -0.10644531\n",
      "  0.00105286 -0.05883789  0.05102539 -0.0291748   0.19335938 -0.14160156\n",
      " -0.33398438  0.08154297 -0.27539062  0.10058594 -0.10449219 -0.12353516\n",
      " -0.140625    0.03491211 -0.11767578 -0.1796875  -0.21484375 -0.23828125\n",
      "  0.08447266 -0.07519531 -0.25976562 -0.21289062 -0.22363281 -0.09716797\n",
      "  0.11572266  0.15429688  0.07373047 -0.27539062  0.14257812 -0.0201416\n",
      "  0.10009766 -0.19042969 -0.09375     0.14160156  0.17089844  0.3125\n",
      " -0.16699219 -0.08691406 -0.05004883 -0.24902344 -0.20800781 -0.09423828\n",
      " -0.12255859 -0.09472656 -0.390625   -0.06640625 -0.31640625  0.10986328\n",
      " -0.00156403  0.04345703  0.15625    -0.18945312 -0.03491211  0.03393555\n",
      " -0.14453125  0.01611328 -0.14160156 -0.02392578  0.01501465  0.07568359\n",
      "  0.10742188  0.12695312  0.10693359 -0.01184082 -0.24023438  0.0291748\n",
      "  0.16210938  0.19921875 -0.28125     0.16699219 -0.11621094 -0.25585938\n",
      "  0.38671875 -0.06640625 -0.4609375  -0.06176758 -0.14453125 -0.11621094\n",
      "  0.05688477  0.03588867 -0.10693359  0.18847656 -0.16699219 -0.01794434\n",
      "  0.10986328 -0.12353516 -0.16308594 -0.14453125  0.12890625  0.11523438\n",
      "  0.13671875  0.05688477 -0.08105469 -0.06152344 -0.06689453  0.27929688\n",
      " -0.19628906  0.07226562  0.12304688 -0.20996094 -0.22070312  0.21386719\n",
      " -0.1484375  -0.05932617  0.05224609  0.06445312 -0.02636719  0.13183594\n",
      "  0.19433594  0.27148438  0.18652344  0.140625    0.06542969 -0.14453125\n",
      "  0.05029297  0.08837891  0.12255859  0.26757812  0.0534668  -0.32226562\n",
      " -0.20703125  0.18164062  0.04418945 -0.22167969 -0.13769531 -0.04174805\n",
      " -0.00286865  0.04077148  0.07275391 -0.08300781  0.08398438 -0.3359375\n",
      " -0.40039062  0.01757812 -0.18652344 -0.0480957  -0.19140625  0.10107422\n",
      "  0.09277344 -0.30664062 -0.19921875 -0.0168457   0.12207031  0.14648438\n",
      " -0.12890625 -0.23535156 -0.05371094 -0.06640625  0.06884766 -0.03637695\n",
      "  0.2109375  -0.06005859  0.19335938  0.05151367 -0.05322266  0.02893066\n",
      " -0.27539062  0.08447266  0.328125    0.01818848  0.01495361  0.04711914\n",
      "  0.37695312 -0.21875    -0.03393555  0.01116943  0.36914062  0.02160645\n",
      "  0.03466797  0.07275391  0.16015625 -0.16503906 -0.296875    0.15039062\n",
      " -0.29101562  0.13964844  0.00448608  0.171875   -0.21972656  0.09326172\n",
      " -0.19042969  0.01599121 -0.09228516  0.15722656 -0.14160156 -0.0534668\n",
      "  0.03613281  0.23632812 -0.15136719 -0.00689697 -0.27148438 -0.07128906\n",
      " -0.16503906  0.18457031 -0.08398438  0.18554688  0.11669922  0.02758789\n",
      " -0.04760742  0.17871094  0.06542969 -0.03540039  0.22949219  0.02697754\n",
      " -0.09765625  0.26953125  0.08349609 -0.13085938 -0.10107422 -0.00738525\n",
      "  0.07128906  0.14941406 -0.20605469  0.18066406 -0.15820312  0.05932617\n",
      "  0.28710938 -0.04663086  0.15136719  0.4921875  -0.27539062  0.05615234]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liye\\AppData\\Local\\Temp\\ipykernel_7736\\283974067.py:13: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  embedding = model.word_vec('cat')\n"
     ]
    }
   ],
   "source": [
    "# See https://radimrehurek.com/gensim/install.html for gensim\n",
    "# installatio instructions\n",
    "# Download and gunzip the word2vec embeddings from\n",
    "# https://github.com/mmihaltz/word2vec-GoogleNews-vectors/blob/master/GoogleNews-vectors-negative300.bin.gz\n",
    "# The 1.5GB compressed file decompresses to 3.4GB.\n",
    "import gensim\n",
    "\n",
    "# The model is large; >= 8GB of RAM is required\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Let's inspect the embedding for \"cat\"\n",
    "embedding = model.word_vec('cat')\n",
    "print(\"Dimensions: %s\" % embedding.shape)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e1f6735-d447-4307-95d5-59453b12d0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76094574\n",
      "0.17211203\n"
     ]
    }
   ],
   "source": [
    "# The vectors for semantically similar words are more similar than the vectors for semantically dissimilar words\n",
    "print(model.similarity('cat', 'dog'))\n",
    "print(model.similarity('cat', 'sandwich'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcb7a468-2a2f-421d-a6a1-11582d23d2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dog', 0.7762665748596191)]\n"
     ]
    }
   ],
   "source": [
    "# Puppy is to cat as kitten is to...\n",
    "print(model.most_similar(positive=['puppy', 'cat'], negative=['kitten'],topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b77475d-2e4f-49c2-bb59-14e7004df4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('saddles', 0.5282258987426758)\n",
      "('horseman', 0.5179382562637329)\n",
      "('jockey', 0.48861294984817505)\n"
     ]
    }
   ],
   "source": [
    "# Palette is to painter as saddle is to...\n",
    "for i in model.most_similar(positive=['saddle', 'painter'], negative=['palette'], topn=3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a96f7e2-ef6e-4333-8b0e-6966e32e6250",
   "metadata": {},
   "source": [
    "## 4.4　从图像中提取特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b369ad-de66-478c-ad83-5cd1de87c5c4",
   "metadata": {},
   "source": [
    "### 4.4.1　从像素强度中提取特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f31dba-f33c-4565-8d72-f209e7daa027",
   "metadata": {},
   "source": [
    "代码4.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a431524b-89d0-42ab-a16b-61e7cf5f3c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit: 0\n",
      "[[ 0.  0.  5. 13.  9.  1.  0.  0.]\n",
      " [ 0.  0. 13. 15. 10. 15.  5.  0.]\n",
      " [ 0.  3. 15.  2.  0. 11.  8.  0.]\n",
      " [ 0.  4. 12.  0.  0.  8.  8.  0.]\n",
      " [ 0.  5.  8.  0.  0.  9.  8.  0.]\n",
      " [ 0.  4. 11.  0.  1. 12.  7.  0.]\n",
      " [ 0.  2. 14.  5. 10. 12.  0.  0.]\n",
      " [ 0.  0.  6. 13. 10.  0.  0.  0.]]\n",
      "Feature vector:\n",
      " [[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      "  15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "   0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "   0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "print('Digit: %s' % digits.target[0])\n",
    "print(digits.images[0])\n",
    "print('Feature vector:\\n %s' % digits.images[0].reshape(-1, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1848c2c1-1efc-4e6e-b92a-3ff756e4918b",
   "metadata": {},
   "source": [
    "### 4.4.2　使用卷积神经网络激活项作为特征"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a8f66-f1d1-48f9-88a9-f4a61989451f",
   "metadata": {},
   "source": [
    "代码4.18"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c91a996f-9bca-45b4-9d12-511efdc80dbd",
   "metadata": {},
   "source": [
    "import os\n",
    "import caffe\n",
    "import numpy as np\n",
    "CAFFE_DIR = '/your/path/to/caffe'\n",
    "MEAN_PATH = os.path.join(CAFFE_DIR,'python/caffe/imagenet/ilsvrc_2012_mean.npy')\n",
    "PROTOTXT_PATH = os.path.join(CAFFE_DIR,'models/bvlc_reference_caffenet/deploy.prototxt')\n",
    "CAFFEMODEL_PATH = os.path.join(CAFFE_DIR,'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel')\n",
    "IMAGE_PATH = 'data/zipper-1.jpg'\n",
    "\n",
    "# 初始化网络\n",
    "net = caffe.Net(PROTOTXT_PATH, CAFFEMODEL_PATH, caffe.TEST)\n",
    "# 配置一个转换器，对输入图片值缩放到[0,1]，再减去每个通道像素的平均值，并将通道调换到RGB颜色空间\n",
    "# 测试图片需要和训练图片做相同的处理\n",
    "transformer = caffe.io.Transformer({'data':\n",
    "  net.blobs['data'].data.shape})\n",
    "transformer.set_transpose('data', (2, 0, 1))\n",
    "transformer.set_mean('data', np.load(MEAN_PATH).mean(1).mean(1))\n",
    "transformer.set_raw_scale('data', 255)\n",
    "transformer.set_channel_swap('data', (2,1,0))\n",
    "\n",
    "# 加载一张图片\n",
    "net.blobs['data'].reshape(1, 3, 227, 227)\n",
    "net.blobs['data'].data[0] = transformer.preprocess('data',\n",
    "   caffe.io.load_image(IMAGE_PATH))\n",
    "\n",
    "# 向前传播，并打印出\"fc7\"层的激活项\n",
    "net.forward()\n",
    "features = net.blobs['fc7'].data.reshape(-1,)\n",
    "print(features.shape)\n",
    "print(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
